\subsubsection{Модель слабого контроля}
\par
При построении алгоритма интерпретации на основе анализа трендов профилей, полученных в параграфе «Построение разрывных трендов профиля» для данных ПГИ и шумометрии, мы предлагаем использовать модель слабого контроля (WS, weak supervision). Это вероятностная модель машинного обучения без учителя, впервые описанная в \cite{ws} как способ быстрого создания большого количества датасетов без экспертной разметки.
\par
Суть модели заключается в объединении результатов работы слабых разметочных функций (LFs, labelling functions) в итоговую разметку. Разметочные функции – эмпирические либо любые другие правила, позволяющие решать поставленную задачу классификации – это может быть как классический вычислительный алгоритм, так и модель машинного обучения. Преимущество модели заключается в том, что разметочные функции не обязаны по отдельности быть очень точными, покрывать все множество возможных объектов для разметки или даже не противоречить друг другу; в результате обучения модель объединяет все их предсказания в одну разметку.
\par
Авторы \cite{ws} описывают WS как генеративную, или порождающую модель (generative model), то есть как вероятностную модель совместного распределения данных. Разметочные функции описываются как «шумные» - решающие задачу не полностью или неточно, и процесс обучения позволяет избавиться от этого шума, учитывая корреляции разметочных функций.
\par
Опишем вкратце процесс обучения модели на примере решения задачи бинарной классификации. Процесс обучения состоит из минимизации двух функционалов.
\par
\textit{Шаг 1.}
Назовем разметочной функцией $\lambda (x)$ функцию, которая каждому объекту обучающего датасета $S$ ставит в соответствие одну из меток {-1, 0, 1}, где метки -1 и 1 соответствуют двум классам решаемой задачи бинарной классификации, а метка 0 соответствует случаю «недостаточно информации, чтобы принять решение».
\par
Для каждой разметочной функции $\lambda (x)$ введем скалярные вероятности $\alpha, \beta$ как:
\begin{itemize}
    \item[] $\alpha$ - вероятность того, что разметочная функция разметит объект не нулем,
    \item[] $\beta$ - вероятность того, что разметочная функция разметит объект меткой -1 или 1 правильным образом.
\end{itemize}
\par
Разметочные функции порождают разметку $\Lambda$.
\par
На первом шаге подбираются параметры $(\hat{\alpha}, \hat{\beta})$ такие, что
\begin{equation}
    \hat{\alpha},\;\hat{\beta} = \arg\max_{\alpha,\beta}\sum_{x\in S}log\mathbb{P}_{(\Lambda, Y)\sim\mu_{\hat{\alpha}, \hat{\beta}}}\left(\Lambda=\lambda (x)\right)=
    \arg\max_{\alpha,\beta}\sum_{x\in S}log\left(  \sum_{y'\in\{-1,1\}} \mu_{\alpha,\beta} (\lambda(x),y')\right)
\end{equation}
\par
Другими словами, полученные оптимизацией функционала векторы параметров $(\hat{\alpha}, \hat{\beta})$ максимизируют правдоподобие полученной разметки.
\par
Здесь $\mu_{\hat{\alpha}, \hat{\beta}}$ - генеративная модель, позволяющая создавать разметку $Y$ для решения задачи бинарной классификации, используя вероятностные метки $\Lambda$ для классов {-1, 0, 1}.
\par
\textit{Шаг 2.}
После нахождения наиболее хорошо описывающих обучающий датасет наборов коэффициентов $(\hat{\alpha}, \hat{\beta})$ решается задача логистической регрессии с модифицированной лосс-функцией. Подбирается набор весов $\hat{w}$ такой, что
\begin{equation}
    \hat{w} = \arg\min_w L_{\hat{\alpha},\hat{\beta}}\left(w,S\right),
\end{equation}
где
\begin{equation}
    L_{\hat{\alpha},\hat{\beta}}\left(w,S\right) = 
    \frac{1}{|S|}\sum_{x\in S}\mathbb{E}_{(\Lambda, Y)\sim\mu_{\hat{\alpha}, \hat{\beta}}}
    \left[log\left(1+e^{-w^Tf(x)Y}\right)|\Lambda=\lambda(x)\right]
    +\rho ||w||^2
\end{equation}

\par
В этой формуле:
\begin{itemize}
    \item Логарифм под матожиданием – лосс-функция для классической задачи логистической регрессии для случая бинарной классификации с метками {-1, 1}:
    \begin{equation}
    L(Y, w^Tf(x)) = log\left(1+e^{-w^Tf(x)Y}\right)
    \end{equation}
    \item Слагаемое $\rho||w||^2$ – стандартная регуляризация весов.
\end{itemize}
\par
Поскольку разные комбинации функций разметки дают разметку разного качества, функцию потерь логистической регрессии следует скорректировать, чтобы давать более шумной разметке меньший вес. Эта коррекция выполняется с помощью функции $\mathbb{E}_{(\Lambda, Y)\sim\mu_{\hat{\alpha}, \hat{\beta}}}$.
\par
Другими словами, уровень доверия к каждой разметочной функции характеризуется коэффициентами вероятностной модели, которые подбираются в процессе обучения. Можно предложить аналогию, в которой каждая разметочная функция – это «эксперт». «Эксперты» обладают различной квалификацией: могут с различной частотой отвечать на любой вопрос «не знаю» (метка 0) и с различной частотой выдавать положительные (метка 1) и отрицательные (метка -1) ответы. Эти частоты соответствуют вероятностям $(\hat{\alpha}, \hat{\beta})$, введенным в предыдущем параграфе. Модель слабого контроля собирает всех «экспертов» в одном месте и для каждого объекта в датасете задает им вопрос – «что думаете?», а после получения меток от каждого «эксперта» на каждый объект принимает решение с учетом того, как показания «экспертов» соотносились между собой.
\par
В данной работе предлагается взять модель слабого контроля как основу для создания модели-советника – автоматизированного алгоритма интерпретации, предлагающего эксперту варианты интерпретаций для выработки окончательного заключения. Предполагается, что функции разметки могут содержать выжимку из экспертных знаний, широко описанных в литературе, чтобы экспертам не пришлось выполнять рутинную работу. Так как итоговое решение строится с оценкой неопределенности, не требуется жесткого контроля над качеством функций разметки – они должны быть просто достаточно адекватны с точки зрения специалистов.
При построении и реализации алгоритма используются программные модули  рассмотренной модели из библиотеки \textit{snorkel} на языке Python.